import cv2
import numpy as np
import mediapipe as mp
from google.colab.patches import cv2_imshow
from google.colab import files
import subprocess
import json
import os
import requests
import tensorflow as tf
from tensorflow.keras.applications import Xception
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.xception import preprocess_input
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model
import warnings
warnings.filterwarnings('ignore')

# --------------------------
# Função 1: Extrair frames
# --------------------------
def extract_frames(video_path, max_frames=30):
    cap = cv2.VideoCapture(video_path)
    frames = []
    count = 0
    while cap.isOpened() and count < max_frames:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)
        count += 1
    cap.release()
    return frames

# --------------------------
# Função 2: Detectar faces e piscadas
# --------------------------
mp_face_mesh = mp.solutions.face_mesh

def detect_faces_and_micro(frames):
    face_issues = 0
    blink_count = 0

    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1) as face_mesh:

        for idx, frame in enumerate(frames):
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = face_mesh.process(rgb_frame)

            if results.multi_face_landmarks is None:
                face_issues += 1
            else:
                landmarks = results.multi_face_landmarks[0].landmark
                left_eye = [landmarks[i] for i in [386, 385, 380, 374, 368, 387]]
                right_eye = [landmarks[i] for i in [159, 158, 153, 145, 144, 160]]

                def eye_aspect_ratio(eye):
                    A = abs(eye[1].y - eye[5].y)
                    B = abs(eye[2].y - eye[4].y)
                    C = abs(eye[0].x - eye[3].x)
                    return (A + B) / (2.0 * C)

                ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2.0
                if ear < 0.2:
                    blink_count += 1

    return {"face_issues": face_issues, "blink_count": blink_count}

# --------------------------
# Função 3: Estimar nitidez
# --------------------------
def estimate_blurriness(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    fm = cv2.Laplacian(gray, cv2.CV_64F).var()
    return fm

# --------------------------
# Função 4: Jitter entre frames
# --------------------------
def detect_frame_jitter(frames):
    diffs = []
    for i in range(1, len(frames)):
        diff = cv2.absdiff(frames[i], frames[i - 1])
        mean_diff = diff.mean()
        diffs.append(mean_diff)
    return diffs

# --------------------------
# Função 5: FFT (frequência espacial)
# --------------------------
def analyze_fft(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    f = np.fft.fft2(gray)
    fshift = np.fft.fftshift(f)
    magnitude_spectrum = 20 * np.log(np.abs(fshift))
    mean_magnitude = np.mean(magnitude_spectrum)
    return mean_magnitude

# --------------------------
# Função 6: Analisar metadados
# --------------------------
def analyze_metadata(video_path):
    cmd = [
        'ffprobe',
        '-v', 'quiet',
        '-print_format', 'json',
        '-show_format',
        '-show_streams',
        video_path
    ]
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    metadata = json.loads(result.stdout)
    return metadata

# --------------------------
# Função 7: Carregar modelo XceptionNet treinado em deepfakes
# --------------------------
def load_xception_model():
    base_model = Xception(weights='imagenet', include_top=False)
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    predictions = Dense(2, activation='softmax')(x)
    model = Model(inputs=base_model.input, outputs=predictions)
    return model

# Baixar pesos do modelo (simulado aqui)
def download_model_weights(model):
    print("[INFO] Carregando modelo XceptionNet treinado...")
    # Aqui você pode carregar seus próprios pesos treinados
    return model

# Classificar frame com XceptionNet
def classify_frame_with_ai(model, frame):
    img = cv2.resize(frame, (299, 299))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)
    prediction = model.predict(img_array, verbose=0)
    fake_prob = prediction[0][1]
    return fake_prob

# --------------------------
# Função 8: Análise final e decisão
# --------------------------
def analyze_video(video_path, max_frames=30):
    print(f"[INFO] Extraindo até {max_frames} frames do vídeo...")
    frames = extract_frames(video_path, max_frames)

    print("[INFO] Carregando modelo XceptionNet treinado...")
    model = load_xception_model()
    download_model_weights(model)

    print("[INFO] Analisando rostos e microcomportamentos...")
    face_data = detect_faces_and_micro(frames)

    print("[INFO] Calculando nitidez média dos frames...")
    blur_scores = [estimate_blurriness(frame) for frame in frames]
    avg_blur = np.mean(blur_scores)

    print("[INFO] Analisando tremor entre frames...")
    jitter_scores = detect_frame_jitter(frames)
    avg_jitter = np.mean(jitter_scores) if jitter_scores else 0

    print("[INFO] Analisando padrões de frequência (FFT)...")
    fft_scores = [analyze_fft(frame) for frame in frames]
    avg_fft = np.mean(fft_scores)

    print("[INFO] Lendo metadados do arquivo...")
    metadata = analyze_metadata(video_path)

    print("[INFO] Analisando frames com XceptionNet...")
    ai_scores = []
    for frame in frames:
        ai_score = classify_frame_with_ai(model, frame)
        ai_scores.append(ai_score)
    avg_ai_score = np.mean(ai_scores)

    print("\n--- RESULTADO DA ANÁLISE ---")
    print(f"Problemas faciais detectados: {face_data['face_issues']}")
    print(f"Piscadelas detectadas: {face_data['blink_count']}")
    print(f"Nitidez média (quanto maior, mais nítido): {avg_blur:.2f}")
    print(f"Média de jitter entre frames: {avg_jitter:.2f}")
    print(f"Média de frequência (FFT): {avg_fft:.2f}")
    print(f"Média de probabilidade de IA (por frame): {avg_ai_score * 100:.2f}%")

    score = 0
    if face_data['face_issues'] > 5:
        score += 2
    if face_data['blink_count'] < 2:
        score += 1
    if avg_blur < 100:
        score += 1
    if avg_jitter > 10:
        score += 1
    if avg_fft > 200:
        score += 1
    if avg_ai_score > 0.6:
        score += 2  # Peso maior para detecção com IA

    # Analisar metadados
    meta_text = json.dumps(metadata).lower()
    ai_keywords = ["google", "lavf", "ai", "synthetic", "fake"]
    found_keywords = [kw for kw in ai_keywords if kw in meta_text]

    if found_keywords:
        score += 2
        print(f"[Pista encontrada nos metadados]: {', '.join(found_keywords)}")

    print(f"Pontuação total de suspeita (limiar: 4): {score}")

    if score >= 4:
        print("⚠️ Resultado: Provavelmente gerado por IA")
    else:
        print("✅ Resultado: Provavelmente real")

    print("\n--- METADADOS DO VÍDEO ---")
    print(json.dumps(metadata, indent=2))

# --------------------------
# Upload do vídeo
# --------------------------
print("[UPLOAD] Faça upload do seu vídeo:")
uploaded = files.upload()

video_path = next(iter(uploaded))

analyze_video(video_path, max_frames=30)
